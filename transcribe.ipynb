{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e275fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, json, argparse, datetime, math\n",
    "import torch\n",
    "import whisperx\n",
    "import subprocess\n",
    "import tempfile\n",
    "import wave\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d5d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_sec(audio_path: str) -> float:\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(audio_path,'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            return frames / float(rate)\n",
    "    except Exception:\n",
    "        # fallback con ffprobe si no es WAV\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\n",
    "                \"-of\",\"default=noprint_wrappers=1:nokey=1\", audio_path\n",
    "            ]).decode().strip()\n",
    "            return float(out)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def merge_contiguous_turns(segments):\n",
    "    \"\"\"Une segmentos consecutivos del mismo speaker en un único turno.\"\"\"\n",
    "    if not segments: return []\n",
    "    merged = []\n",
    "    cur = dict(speaker=segments[0][\"speaker\"], start=segments[0][\"start\"], end=segments[0][\"end\"], text=segments[0][\"text\"])\n",
    "    for seg in segments[1:]:\n",
    "        if seg[\"speaker\"] == cur[\"speaker\"] and (seg[\"start\"] - cur[\"end\"]) <= 0.6:\n",
    "            # pegar si están pegados o muy cercanos (0.6s)\n",
    "            cur[\"end\"] = seg[\"end\"]\n",
    "            cur[\"text\"] += (\" \" if cur[\"text\"] else \"\") + seg[\"text\"]\n",
    "        else:\n",
    "            merged.append(cur)\n",
    "            cur = dict(speaker=seg[\"speaker\"], start=seg[\"start\"], end=seg[\"end\"], text=seg[\"text\"])\n",
    "    merged.append(cur)\n",
    "    return merged\n",
    "\n",
    "def guess_interviewer(speaker_stats, turns):\n",
    "    # Heurística: entrevistador = quien hace más preguntas y habla menos tiempo.\n",
    "    # 1) contar signos de interrogación y frases interrogativas por speaker\n",
    "    q_words = (\"¿\", \"?\", \"qué\", \"que\", \"quién\", \"quien\", \"cuándo\", \"cuando\", \"dónde\", \"donde\",\n",
    "               \"por qué\", \"por que\", \"cómo\", \"como\", \"cuál\", \"cual\", \"cuáles\", \"cuales\")\n",
    "    q_score = {spk:0 for spk in speaker_stats}\n",
    "    for t in turns:\n",
    "        txt = t[\"text\"].lower()\n",
    "        if any(w in txt for w in q_words):\n",
    "            q_score[t[\"speaker\"]] += 1\n",
    "    # 2) normalizar por tiempo total (quien pregunta más/tiempo) y habla menos\n",
    "    best = None\n",
    "    best_val = -1e9\n",
    "    for spk, st in speaker_stats.items():\n",
    "        time = st[\"total_sec\"]\n",
    "        asks = q_score.get(spk,0)\n",
    "        # más preguntas por minuto y menos tiempo total => mayor score\n",
    "        val = (asks / max(time,1e-6)) - 0.001*time\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best = spk\n",
    "    return best\n",
    "\n",
    "def build_qa(turns, interviewer):\n",
    "    \"\"\"Forma pares Q->A: pregunta del entrevistador y respuesta(s) hasta que el entrevistador hable de nuevo.\"\"\"\n",
    "    qa = []\n",
    "    i = 0\n",
    "    while i < len(turns):\n",
    "        t = turns[i]\n",
    "        if t[\"speaker\"] == interviewer and (\"?\" in t[\"text\"] or \"¿\" in t[\"text\"]):\n",
    "            q = {\n",
    "                \"q_speaker\": interviewer,\n",
    "                \"q_start\": t[\"start\"],\n",
    "                \"q_end\": t[\"end\"],\n",
    "                \"question\": t[\"text\"].strip(),\n",
    "                \"answers\": []\n",
    "            }\n",
    "            i += 1\n",
    "            # recolecta todas las réplicas de otros speakers hasta que vuelva a hablar el entrevistador\n",
    "            while i < len(turns) and turns[i][\"speaker\"] != interviewer:\n",
    "                a = turns[i]\n",
    "                if a[\"text\"].strip():\n",
    "                    q[\"answers\"].append({\n",
    "                        \"a_speaker\": a[\"speaker\"],\n",
    "                        \"a_start\": a[\"start\"],\n",
    "                        \"a_end\": a[\"end\"],\n",
    "                        \"answer\": a[\"text\"].strip()\n",
    "                    })\n",
    "                i += 1\n",
    "            qa.append(q)\n",
    "        else:\n",
    "            i += 1\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = os.getenv(\"DRIVE_PATH\")\n",
    "if not drive_path:\n",
    "    raise ValueError(\"Debe definir la variable de entorno DRIVE_PATH con la ruta a Google Drive.\")\n",
    "# Filtrar solo archivos con extensiones de audio o video comunes\n",
    "audio_video_exts = ('.mp3', '.aac', '.m4a', '.mp4', '.wav')\n",
    "drive_files = [f for f in os.listdir(drive_path) if f.lower().endswith(audio_video_exts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4d16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Configurando parámetros...\n",
      "🔹 Usando device: cpu\n",
      "🔹 Procesando archivo: /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Parámetros configurables como variables\n",
    "print(\"🔹 Configurando parámetros...\")\n",
    "model_name = \"large-v3\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# --- Elección de dispositivos por etapa ---\n",
    "has_cuda = torch.cuda.is_available()\n",
    "has_mps  = torch.backends.mps.is_available()\n",
    "\n",
    "# ASR (WhisperX/Faster-Whisper via CTranslate2) -> NO soporta MPS\n",
    "asr_device      = \"cuda\" if has_cuda else \"cpu\"\n",
    "asr_compute     = \"float16\" if has_cuda else \"int8\"   # int8 va bien en CPU\n",
    "\n",
    "# Alineación (MFA/CTranslate2) -> mismo que ASR\n",
    "align_device    = asr_device\n",
    "\n",
    "# Diarización (pyannote, PyTorch) -> puede usar MPS, si no CPU\n",
    "diar_device     = \"mps\" if has_mps else (\"cuda\" if has_cuda else \"cpu\")\n",
    "\n",
    "# (Opcional) hilos CPU para CTranslate2\n",
    "asr_threads = max(1, os.cpu_count() // 2)\n",
    "\n",
    "print(f\"🔹 Usando device: {asr_device}\")\n",
    "\n",
    "language = \"es\"  # Cambia si necesitas otro idioma\n",
    "\n",
    "audio_path = os.path.join(drive_path, drive_files[0])\n",
    "out_path = os.path.splitext(audio_path)[0] + \".json\"\n",
    "\n",
    "print(f\"🔹 Procesando archivo: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9895560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Cargando modelo WhisperX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint .venv/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0. Bad things might happen unless you revert torch to 1.x.\n",
      "🔹 Transcribiendo audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoramirez/Dev/settlements-surveys/.venv/lib/python3.10/site-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    }
   ],
   "source": [
    "# 1) Transcribir\n",
    "print(\"🔹 Cargando modelo WhisperX...\")\n",
    "model = whisperx.load_model(model_name, device=asr_device, compute_type=asr_compute, language=language)\n",
    "print(\"🔹 Transcribiendo audio...\")\n",
    "asr_result = model.transcribe(audio_path, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0b49bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Cargando modelo de alineación...\n",
      "🔹 Realizando alineación...\n"
     ]
    }
   ],
   "source": [
    "# 2) Alinear palabras a tiempo exacto\n",
    "print(\"🔹 Cargando modelo de alineación...\")\n",
    "align_model, metadata = whisperx.load_align_model(language_code=asr_result[\"language\"], device=align_device)\n",
    "print(\"🔹 Realizando alineación...\")\n",
    "aligned = whisperx.align(asr_result[\"segments\"], align_model, metadata, audio_path, align_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b483c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Ejecutando diarización...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c938ae32f44208973d4ba3f26792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0d7322c0f64760a9cc96125965e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoramirez/Dev/settlements-surveys/.venv/lib/python3.10/site-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca78574d4ec489dbe03a9e1d2012c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a7c584133a456587c0a8b9c1d752f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Diarizar (requiere HF token y haber aceptado el modelo en HF)\n",
    "print(\"🔹 Ejecutando diarización...\")\n",
    "from whisperx.diarize import DiarizationPipeline\n",
    "diarize_pipeline = DiarizationPipeline(use_auth_token=hf_token, device=diar_device)\n",
    "diarize_segments = diarize_pipeline(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3727474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Asignando hablantes a palabras...\n"
     ]
    }
   ],
   "source": [
    "# 4) Asignar hablantes a palabras y recomponer segmentos\n",
    "print(\"🔹 Asignando hablantes a palabras...\")\n",
    "diarized = whisperx.assign_word_speakers(diarize_segments, aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f4fe34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Construyendo segmentos...\n",
      "🔹 Uniendo tramos contiguos del mismo hablante...\n",
      "🔹 Calculando estadísticas por hablante...\n",
      "🔹 Detectando entrevistador...\n",
      "🔹 Marcando entrevistador en estadísticas...\n",
      "🔹 Construyendo pares Q→A...\n",
      "🔹 Calculando metadata...\n",
      "🔹 Guardando resultados en /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.json ...\n",
      "✅ Listo: /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.json\n"
     ]
    }
   ],
   "source": [
    "# Construir lista de segmentos (speaker, start, end, text)\n",
    "print(\"🔹 Construyendo segmentos...\")\n",
    "segs = []\n",
    "for seg in diarized[\"segments\"]:\n",
    "    if not seg.get(\"words\"):\n",
    "        continue\n",
    "    words = [w for w in seg[\"words\"] if \"start\" in w and \"end\" in w and \"speaker\" in w]\n",
    "    if not words:\n",
    "        continue\n",
    "    current_spk = words[0][\"speaker\"]\n",
    "    current_start = words[0][\"start\"]\n",
    "    current_text = []\n",
    "    for w in words:\n",
    "        if w[\"speaker\"] != current_spk:\n",
    "            segs.append({\n",
    "                \"speaker\": current_spk,\n",
    "                \"start\": current_start,\n",
    "                \"end\": prev_end,\n",
    "                \"text\": \" \".join(current_text).strip()\n",
    "            })\n",
    "            current_spk = w[\"speaker\"]\n",
    "            current_start = w[\"start\"]\n",
    "            current_text = [w.get(\"word\",\"\")]\n",
    "        else:\n",
    "            current_text.append(w.get(\"word\",\"\"))\n",
    "        prev_end = w[\"end\"]\n",
    "    segs.append({\n",
    "        \"speaker\": current_spk,\n",
    "        \"start\": current_start,\n",
    "        \"end\": prev_end,\n",
    "        \"text\": \" \".join(current_text).strip()\n",
    "    })\n",
    "\n",
    "print(\"🔹 Uniendo tramos contiguos del mismo hablante...\")\n",
    "turns = merge_contiguous_turns(sorted(segs, key=lambda x: (x[\"start\"], x[\"end\"])))\n",
    "\n",
    "print(\"🔹 Calculando estadísticas por hablante...\")\n",
    "speaker_stats = {}\n",
    "for t in turns:\n",
    "    d = t[\"end\"] - t[\"start\"]\n",
    "    spk = t[\"speaker\"]\n",
    "    if spk not in speaker_stats:\n",
    "        speaker_stats[spk] = {\"total_sec\":0.0, \"num_utts\":0}\n",
    "    speaker_stats[spk][\"total_sec\"] += max(0.0, d)\n",
    "    speaker_stats[spk][\"num_utts\"] += 1\n",
    "\n",
    "print(\"🔹 Detectando entrevistador...\")\n",
    "interviewer = guess_interviewer(speaker_stats, turns)\n",
    "\n",
    "print(\"🔹 Marcando entrevistador en estadísticas...\")\n",
    "speakers_list = []\n",
    "for spk, st in sorted(speaker_stats.items()):\n",
    "    speakers_list.append({\n",
    "        \"id\": spk,\n",
    "        \"total_sec\": round(st[\"total_sec\"], 3),\n",
    "        \"num_utts\": st[\"num_utts\"],\n",
    "        \"is_interviewer\": (spk == interviewer)\n",
    "    })\n",
    "\n",
    "print(\"🔹 Construyendo pares Q→A...\")\n",
    "qa = build_qa(turns, interviewer)\n",
    "\n",
    "print(\"🔹 Calculando metadata...\")\n",
    "duration = get_duration_sec(audio_path) or 0\n",
    "result_json = {\n",
    "    \"meta\": {\n",
    "        \"source_audio\": os.path.abspath(audio_path),\n",
    "        \"language\": asr_result['language'],\n",
    "        \"duration_sec\": round(duration, 3),\n",
    "        \"created_utc\": datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\",\n",
    "        \"toolchain\": {\n",
    "            \"asr\": f\"whisperx-{model_name}\",\n",
    "            \"diarization\": \"pyannote\",\n",
    "            \"alignment\": \"mfa\"\n",
    "        }\n",
    "    },\n",
    "    \"speakers\": speakers_list,\n",
    "    \"turns\": [\n",
    "        {\n",
    "            \"speaker\": t[\"speaker\"],\n",
    "            \"start\": round(t[\"start\"], 3),\n",
    "            \"end\": round(t[\"end\"], 3),\n",
    "            \"text\": t[\"text\"]\n",
    "        } for t in turns if t[\"text\"]\n",
    "    ],\n",
    "    \"qa\": qa\n",
    "}\n",
    "\n",
    "print(f\"🔹 Guardando resultados en {out_path} ...\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Listo: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f5e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
