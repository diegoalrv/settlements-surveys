{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e275fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, json, argparse, datetime, math\n",
    "import torch\n",
    "import whisperx\n",
    "import subprocess\n",
    "import tempfile\n",
    "import wave\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d5d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_sec(audio_path: str) -> float:\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(audio_path,'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            return frames / float(rate)\n",
    "    except Exception:\n",
    "        # fallback con ffprobe si no es WAV\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\n",
    "                \"-of\",\"default=noprint_wrappers=1:nokey=1\", audio_path\n",
    "            ]).decode().strip()\n",
    "            return float(out)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def merge_contiguous_turns(segments):\n",
    "    \"\"\"Une segmentos consecutivos del mismo speaker en un Ãºnico turno.\"\"\"\n",
    "    if not segments: return []\n",
    "    merged = []\n",
    "    cur = dict(speaker=segments[0][\"speaker\"], start=segments[0][\"start\"], end=segments[0][\"end\"], text=segments[0][\"text\"])\n",
    "    for seg in segments[1:]:\n",
    "        if seg[\"speaker\"] == cur[\"speaker\"] and (seg[\"start\"] - cur[\"end\"]) <= 0.6:\n",
    "            # pegar si estÃ¡n pegados o muy cercanos (0.6s)\n",
    "            cur[\"end\"] = seg[\"end\"]\n",
    "            cur[\"text\"] += (\" \" if cur[\"text\"] else \"\") + seg[\"text\"]\n",
    "        else:\n",
    "            merged.append(cur)\n",
    "            cur = dict(speaker=seg[\"speaker\"], start=seg[\"start\"], end=seg[\"end\"], text=seg[\"text\"])\n",
    "    merged.append(cur)\n",
    "    return merged\n",
    "\n",
    "def guess_interviewer(speaker_stats, turns):\n",
    "    # HeurÃ­stica: entrevistador = quien hace mÃ¡s preguntas y habla menos tiempo.\n",
    "    # 1) contar signos de interrogaciÃ³n y frases interrogativas por speaker\n",
    "    q_words = (\"Â¿\", \"?\", \"quÃ©\", \"que\", \"quiÃ©n\", \"quien\", \"cuÃ¡ndo\", \"cuando\", \"dÃ³nde\", \"donde\",\n",
    "               \"por quÃ©\", \"por que\", \"cÃ³mo\", \"como\", \"cuÃ¡l\", \"cual\", \"cuÃ¡les\", \"cuales\")\n",
    "    q_score = {spk:0 for spk in speaker_stats}\n",
    "    for t in turns:\n",
    "        txt = t[\"text\"].lower()\n",
    "        if any(w in txt for w in q_words):\n",
    "            q_score[t[\"speaker\"]] += 1\n",
    "    # 2) normalizar por tiempo total (quien pregunta mÃ¡s/tiempo) y habla menos\n",
    "    best = None\n",
    "    best_val = -1e9\n",
    "    for spk, st in speaker_stats.items():\n",
    "        time = st[\"total_sec\"]\n",
    "        asks = q_score.get(spk,0)\n",
    "        # mÃ¡s preguntas por minuto y menos tiempo total => mayor score\n",
    "        val = (asks / max(time,1e-6)) - 0.001*time\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best = spk\n",
    "    return best\n",
    "\n",
    "def build_qa(turns, interviewer):\n",
    "    \"\"\"Forma pares Q->A: pregunta del entrevistador y respuesta(s) hasta que el entrevistador hable de nuevo.\"\"\"\n",
    "    qa = []\n",
    "    i = 0\n",
    "    while i < len(turns):\n",
    "        t = turns[i]\n",
    "        if t[\"speaker\"] == interviewer and (\"?\" in t[\"text\"] or \"Â¿\" in t[\"text\"]):\n",
    "            q = {\n",
    "                \"q_speaker\": interviewer,\n",
    "                \"q_start\": t[\"start\"],\n",
    "                \"q_end\": t[\"end\"],\n",
    "                \"question\": t[\"text\"].strip(),\n",
    "                \"answers\": []\n",
    "            }\n",
    "            i += 1\n",
    "            # recolecta todas las rÃ©plicas de otros speakers hasta que vuelva a hablar el entrevistador\n",
    "            while i < len(turns) and turns[i][\"speaker\"] != interviewer:\n",
    "                a = turns[i]\n",
    "                if a[\"text\"].strip():\n",
    "                    q[\"answers\"].append({\n",
    "                        \"a_speaker\": a[\"speaker\"],\n",
    "                        \"a_start\": a[\"start\"],\n",
    "                        \"a_end\": a[\"end\"],\n",
    "                        \"answer\": a[\"text\"].strip()\n",
    "                    })\n",
    "                i += 1\n",
    "            qa.append(q)\n",
    "        else:\n",
    "            i += 1\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = os.getenv(\"DRIVE_PATH\")\n",
    "if not drive_path:\n",
    "    raise ValueError(\"Debe definir la variable de entorno DRIVE_PATH con la ruta a Google Drive.\")\n",
    "# Filtrar solo archivos con extensiones de audio o video comunes\n",
    "audio_video_exts = ('.mp3', '.aac', '.m4a', '.mp4', '.wav')\n",
    "drive_files = [f for f in os.listdir(drive_path) if f.lower().endswith(audio_video_exts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4d16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Configurando parÃ¡metros...\n",
      "ðŸ”¹ Usando device: cpu\n",
      "ðŸ”¹ Procesando archivo: /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# ParÃ¡metros configurables como variables\n",
    "print(\"ðŸ”¹ Configurando parÃ¡metros...\")\n",
    "model_name = \"large-v3\"\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# --- ElecciÃ³n de dispositivos por etapa ---\n",
    "has_cuda = torch.cuda.is_available()\n",
    "has_mps  = torch.backends.mps.is_available()\n",
    "\n",
    "# ASR (WhisperX/Faster-Whisper via CTranslate2) -> NO soporta MPS\n",
    "asr_device      = \"cuda\" if has_cuda else \"cpu\"\n",
    "asr_compute     = \"float16\" if has_cuda else \"int8\"   # int8 va bien en CPU\n",
    "\n",
    "# AlineaciÃ³n (MFA/CTranslate2) -> mismo que ASR\n",
    "align_device    = asr_device\n",
    "\n",
    "# DiarizaciÃ³n (pyannote, PyTorch) -> puede usar MPS, si no CPU\n",
    "diar_device     = \"mps\" if has_mps else (\"cuda\" if has_cuda else \"cpu\")\n",
    "\n",
    "# (Opcional) hilos CPU para CTranslate2\n",
    "asr_threads = max(1, os.cpu_count() // 2)\n",
    "\n",
    "print(f\"ðŸ”¹ Usando device: {asr_device}\")\n",
    "\n",
    "language = \"es\"  # Cambia si necesitas otro idioma\n",
    "\n",
    "audio_path = os.path.join(drive_path, drive_files[0])\n",
    "out_path = os.path.splitext(audio_path)[0] + \".json\"\n",
    "\n",
    "print(f\"ðŸ”¹ Procesando archivo: {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9895560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Cargando modelo WhisperX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint .venv/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.8.0. Bad things might happen unless you revert torch to 1.x.\n",
      "ðŸ”¹ Transcribiendo audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoramirez/Dev/settlements-surveys/.venv/lib/python3.10/site-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    }
   ],
   "source": [
    "# 1) Transcribir\n",
    "print(\"ðŸ”¹ Cargando modelo WhisperX...\")\n",
    "model = whisperx.load_model(model_name, device=asr_device, compute_type=asr_compute, language=language)\n",
    "print(\"ðŸ”¹ Transcribiendo audio...\")\n",
    "asr_result = model.transcribe(audio_path, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0b49bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Cargando modelo de alineaciÃ³n...\n",
      "ðŸ”¹ Realizando alineaciÃ³n...\n"
     ]
    }
   ],
   "source": [
    "# 2) Alinear palabras a tiempo exacto\n",
    "print(\"ðŸ”¹ Cargando modelo de alineaciÃ³n...\")\n",
    "align_model, metadata = whisperx.load_align_model(language_code=asr_result[\"language\"], device=align_device)\n",
    "print(\"ðŸ”¹ Realizando alineaciÃ³n...\")\n",
    "aligned = whisperx.align(asr_result[\"segments\"], align_model, metadata, audio_path, align_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b483c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Ejecutando diarizaciÃ³n...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c938ae32f44208973d4ba3f26792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0d7322c0f64760a9cc96125965e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoramirez/Dev/settlements-surveys/.venv/lib/python3.10/site-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  torchaudio.list_audio_backends()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca78574d4ec489dbe03a9e1d2012c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/26.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a7c584133a456587c0a8b9c1d752f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3) Diarizar (requiere HF token y haber aceptado el modelo en HF)\n",
    "print(\"ðŸ”¹ Ejecutando diarizaciÃ³n...\")\n",
    "from whisperx.diarize import DiarizationPipeline\n",
    "diarize_pipeline = DiarizationPipeline(use_auth_token=hf_token, device=diar_device)\n",
    "diarize_segments = diarize_pipeline(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3727474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Asignando hablantes a palabras...\n"
     ]
    }
   ],
   "source": [
    "# 4) Asignar hablantes a palabras y recomponer segmentos\n",
    "print(\"ðŸ”¹ Asignando hablantes a palabras...\")\n",
    "diarized = whisperx.assign_word_speakers(diarize_segments, aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f4fe34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Construyendo segmentos...\n",
      "ðŸ”¹ Uniendo tramos contiguos del mismo hablante...\n",
      "ðŸ”¹ Calculando estadÃ­sticas por hablante...\n",
      "ðŸ”¹ Detectando entrevistador...\n",
      "ðŸ”¹ Marcando entrevistador en estadÃ­sticas...\n",
      "ðŸ”¹ Construyendo pares Qâ†’A...\n",
      "ðŸ”¹ Calculando metadata...\n",
      "ðŸ”¹ Guardando resultados en /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.json ...\n",
      "âœ… Listo: /Users/diegoramirez/Google Drive/Unidades compartidas/City Science Network - Challenges/2. Informal Settlements/Datos/Audios/Maria Reyes.json\n"
     ]
    }
   ],
   "source": [
    "# Construir lista de segmentos (speaker, start, end, text)\n",
    "print(\"ðŸ”¹ Construyendo segmentos...\")\n",
    "segs = []\n",
    "for seg in diarized[\"segments\"]:\n",
    "    if not seg.get(\"words\"):\n",
    "        continue\n",
    "    words = [w for w in seg[\"words\"] if \"start\" in w and \"end\" in w and \"speaker\" in w]\n",
    "    if not words:\n",
    "        continue\n",
    "    current_spk = words[0][\"speaker\"]\n",
    "    current_start = words[0][\"start\"]\n",
    "    current_text = []\n",
    "    for w in words:\n",
    "        if w[\"speaker\"] != current_spk:\n",
    "            segs.append({\n",
    "                \"speaker\": current_spk,\n",
    "                \"start\": current_start,\n",
    "                \"end\": prev_end,\n",
    "                \"text\": \" \".join(current_text).strip()\n",
    "            })\n",
    "            current_spk = w[\"speaker\"]\n",
    "            current_start = w[\"start\"]\n",
    "            current_text = [w.get(\"word\",\"\")]\n",
    "        else:\n",
    "            current_text.append(w.get(\"word\",\"\"))\n",
    "        prev_end = w[\"end\"]\n",
    "    segs.append({\n",
    "        \"speaker\": current_spk,\n",
    "        \"start\": current_start,\n",
    "        \"end\": prev_end,\n",
    "        \"text\": \" \".join(current_text).strip()\n",
    "    })\n",
    "\n",
    "print(\"ðŸ”¹ Uniendo tramos contiguos del mismo hablante...\")\n",
    "turns = merge_contiguous_turns(sorted(segs, key=lambda x: (x[\"start\"], x[\"end\"])))\n",
    "\n",
    "print(\"ðŸ”¹ Calculando estadÃ­sticas por hablante...\")\n",
    "speaker_stats = {}\n",
    "for t in turns:\n",
    "    d = t[\"end\"] - t[\"start\"]\n",
    "    spk = t[\"speaker\"]\n",
    "    if spk not in speaker_stats:\n",
    "        speaker_stats[spk] = {\"total_sec\":0.0, \"num_utts\":0}\n",
    "    speaker_stats[spk][\"total_sec\"] += max(0.0, d)\n",
    "    speaker_stats[spk][\"num_utts\"] += 1\n",
    "\n",
    "print(\"ðŸ”¹ Detectando entrevistador...\")\n",
    "interviewer = guess_interviewer(speaker_stats, turns)\n",
    "\n",
    "print(\"ðŸ”¹ Marcando entrevistador en estadÃ­sticas...\")\n",
    "speakers_list = []\n",
    "for spk, st in sorted(speaker_stats.items()):\n",
    "    speakers_list.append({\n",
    "        \"id\": spk,\n",
    "        \"total_sec\": round(st[\"total_sec\"], 3),\n",
    "        \"num_utts\": st[\"num_utts\"],\n",
    "        \"is_interviewer\": (spk == interviewer)\n",
    "    })\n",
    "\n",
    "print(\"ðŸ”¹ Construyendo pares Qâ†’A...\")\n",
    "qa = build_qa(turns, interviewer)\n",
    "\n",
    "print(\"ðŸ”¹ Calculando metadata...\")\n",
    "duration = get_duration_sec(audio_path) or 0\n",
    "result_json = {\n",
    "    \"meta\": {\n",
    "        \"source_audio\": os.path.abspath(audio_path),\n",
    "        \"language\": asr_result['language'],\n",
    "        \"duration_sec\": round(duration, 3),\n",
    "        \"created_utc\": datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\",\n",
    "        \"toolchain\": {\n",
    "            \"asr\": f\"whisperx-{model_name}\",\n",
    "            \"diarization\": \"pyannote\",\n",
    "            \"alignment\": \"mfa\"\n",
    "        }\n",
    "    },\n",
    "    \"speakers\": speakers_list,\n",
    "    \"turns\": [\n",
    "        {\n",
    "            \"speaker\": t[\"speaker\"],\n",
    "            \"start\": round(t[\"start\"], 3),\n",
    "            \"end\": round(t[\"end\"], 3),\n",
    "            \"text\": t[\"text\"]\n",
    "        } for t in turns if t[\"text\"]\n",
    "    ],\n",
    "    \"qa\": qa\n",
    "}\n",
    "\n",
    "print(f\"ðŸ”¹ Guardando resultados en {out_path} ...\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Listo: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f5e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
