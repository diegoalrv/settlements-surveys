{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e275fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, json, argparse, datetime, math\n",
    "import torch\n",
    "import whisperx\n",
    "import subprocess\n",
    "import tempfile\n",
    "import wave\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d5d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_sec(audio_path: str) -> float:\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(audio_path,'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            return frames / float(rate)\n",
    "    except Exception:\n",
    "        # fallback con ffprobe si no es WAV\n",
    "        try:\n",
    "            out = subprocess.check_output([\n",
    "                \"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\n",
    "                \"-of\",\"default=noprint_wrappers=1:nokey=1\", audio_path\n",
    "            ]).decode().strip()\n",
    "            return float(out)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def merge_contiguous_turns(segments):\n",
    "    \"\"\"Une segmentos consecutivos del mismo speaker en un único turno.\"\"\"\n",
    "    if not segments: return []\n",
    "    merged = []\n",
    "    cur = dict(speaker=segments[0][\"speaker\"], start=segments[0][\"start\"], end=segments[0][\"end\"], text=segments[0][\"text\"])\n",
    "    for seg in segments[1:]:\n",
    "        if seg[\"speaker\"] == cur[\"speaker\"] and (seg[\"start\"] - cur[\"end\"]) <= 0.6:\n",
    "            # pegar si están pegados o muy cercanos (0.6s)\n",
    "            cur[\"end\"] = seg[\"end\"]\n",
    "            cur[\"text\"] += (\" \" if cur[\"text\"] else \"\") + seg[\"text\"]\n",
    "        else:\n",
    "            merged.append(cur)\n",
    "            cur = dict(speaker=seg[\"speaker\"], start=seg[\"start\"], end=seg[\"end\"], text=seg[\"text\"])\n",
    "    merged.append(cur)\n",
    "    return merged\n",
    "\n",
    "def guess_interviewer(speaker_stats, turns):\n",
    "    # Heurística: entrevistador = quien hace más preguntas y habla menos tiempo.\n",
    "    # 1) contar signos de interrogación y frases interrogativas por speaker\n",
    "    q_words = (\"¿\", \"?\", \"qué\", \"que\", \"quién\", \"quien\", \"cuándo\", \"cuando\", \"dónde\", \"donde\",\n",
    "               \"por qué\", \"por que\", \"cómo\", \"como\", \"cuál\", \"cual\", \"cuáles\", \"cuales\")\n",
    "    q_score = {spk:0 for spk in speaker_stats}\n",
    "    for t in turns:\n",
    "        txt = t[\"text\"].lower()\n",
    "        if any(w in txt for w in q_words):\n",
    "            q_score[t[\"speaker\"]] += 1\n",
    "    # 2) normalizar por tiempo total (quien pregunta más/tiempo) y habla menos\n",
    "    best = None\n",
    "    best_val = -1e9\n",
    "    for spk, st in speaker_stats.items():\n",
    "        time = st[\"total_sec\"]\n",
    "        asks = q_score.get(spk,0)\n",
    "        # más preguntas por minuto y menos tiempo total => mayor score\n",
    "        val = (asks / max(time,1e-6)) - 0.001*time\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best = spk\n",
    "    return best\n",
    "\n",
    "def build_qa(turns, interviewer):\n",
    "    \"\"\"Forma pares Q->A: pregunta del entrevistador y respuesta(s) hasta que el entrevistador hable de nuevo.\"\"\"\n",
    "    qa = []\n",
    "    i = 0\n",
    "    while i < len(turns):\n",
    "        t = turns[i]\n",
    "        if t[\"speaker\"] == interviewer and (\"?\" in t[\"text\"] or \"¿\" in t[\"text\"]):\n",
    "            q = {\n",
    "                \"q_speaker\": interviewer,\n",
    "                \"q_start\": t[\"start\"],\n",
    "                \"q_end\": t[\"end\"],\n",
    "                \"question\": t[\"text\"].strip(),\n",
    "                \"answers\": []\n",
    "            }\n",
    "            i += 1\n",
    "            # recolecta todas las réplicas de otros speakers hasta que vuelva a hablar el entrevistador\n",
    "            while i < len(turns) and turns[i][\"speaker\"] != interviewer:\n",
    "                a = turns[i]\n",
    "                if a[\"text\"].strip():\n",
    "                    q[\"answers\"].append({\n",
    "                        \"a_speaker\": a[\"speaker\"],\n",
    "                        \"a_start\": a[\"start\"],\n",
    "                        \"a_end\": a[\"end\"],\n",
    "                        \"answer\": a[\"text\"].strip()\n",
    "                    })\n",
    "                i += 1\n",
    "            qa.append(q)\n",
    "        else:\n",
    "            i += 1\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = os.getenv(\"DRIVE_PATH\")\n",
    "if not drive_path:\n",
    "    raise ValueError(\"Debe definir la variable de entorno DRIVE_PATH con la ruta a Google Drive.\")\n",
    "drive_files = os.listdir(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f8d5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maria Reyes.mp3',\n",
       " '60.m4a',\n",
       " 'Antonio Jael.m4a',\n",
       " '77.m4a',\n",
       " 'Yam jamett.m4a',\n",
       " 'Casa 78.m4a',\n",
       " 'Brigit Velasquez.mp4',\n",
       " 'Casa 91.m4a',\n",
       " 'Jessica Ulcuango.mp4',\n",
       " 'Cecilia Corona.mp3',\n",
       " 'Casa 68.m4a',\n",
       " 'Maryory Valestrini.mp3',\n",
       " 'Milexi Rodriguez 73.m4a',\n",
       " 'Casa 71.m4a',\n",
       " 'Casa 73.m4a',\n",
       " 'Maria Fica.m4a',\n",
       " 'Maryoribeth Isea.mp4',\n",
       " 'Jonathan Pacheco.m4a',\n",
       " 'Maria Acosta.m4a',\n",
       " 'Tamara Lara.mp4',\n",
       " '75 -1.m4a',\n",
       " '69.m4a',\n",
       " 'Nicolas Montanares.mp4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fe34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"audio\", help=\"Ruta del archivo de audio (wav/mp3/m4a/etc.)\")\n",
    "# ap.add_argument(\"-o\",\"--out\", help=\"Ruta del JSON de salida\", default=None)\n",
    "# ap.add_argument(\"--model\", default=\"large-v3\", help=\"Modelo WhisperX (p.ej., large-v3, medium, small)\")\n",
    "# ap.add_argument(\"--lang\", default=\"es\", help=\"Idioma esperado (es, en, etc.)\")\n",
    "# ap.add_argument(\"--hf_token\", default=os.getenv(\"HF_TOKEN\"), help=\"Token de Hugging Face para diarización\")\n",
    "# ap.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = ap.parse_args()\n",
    "\n",
    "# audio_path = args.audio\n",
    "audio_path = os.path.join(drive_path, drive_files[0])\n",
    "out_path = args.out or (os.path.splitext(audio_path)[0] + \".json\")\n",
    "\n",
    "device = args.device\n",
    "compute_type = \"float16\" if (device==\"cuda\") else \"int8\"\n",
    "\n",
    "# 1) Transcribir\n",
    "model = whisperx.load_model(args.model, device=device, compute_type=compute_type, language=args.lang)\n",
    "asr_result = model.transcribe(audio_path, batch_size=16)\n",
    "\n",
    "# 2) Alinear palabras a tiempo exacto\n",
    "align_model, metadata = whisperx.load_align_model(language_code=asr_result[\"language\"], device=device)\n",
    "aligned = whisperx.align(asr_result[\"segments\"], align_model, metadata, audio_path, device)\n",
    "\n",
    "# 3) Diarizar (requiere HF token y haber aceptado el modelo en HF)\n",
    "diarize_pipeline = whisperx.DiarizationPipeline(use_auth_token=args.hf_token, device=device)\n",
    "diarize_segments = diarize_pipeline(audio_path)\n",
    "\n",
    "# 4) Asignar hablantes a palabras y recomponer segmentos\n",
    "diarized = whisperx.assign_word_speakers(diarize_segments, aligned)\n",
    "\n",
    "# Construir lista de segmentos (speaker, start, end, text)\n",
    "# whisperx deja \"segments\" con palabras alineadas y speaker en \"words\"\n",
    "segs = []\n",
    "for seg in diarized[\"segments\"]:\n",
    "    # calcular ventana por min/max de words (start/end)\n",
    "    if not seg.get(\"words\"):\n",
    "        continue\n",
    "    words = [w for w in seg[\"words\"] if \"start\" in w and \"end\" in w and \"speaker\" in w]\n",
    "    if not words:\n",
    "        continue\n",
    "    # partir por cambios de speaker dentro del segmento\n",
    "    current_spk = words[0][\"speaker\"]\n",
    "    current_start = words[0][\"start\"]\n",
    "    current_text = []\n",
    "    for w in words:\n",
    "        if w[\"speaker\"] != current_spk:\n",
    "            # cerrar tramo anterior\n",
    "            segs.append({\n",
    "                \"speaker\": current_spk,\n",
    "                \"start\": current_start,\n",
    "                \"end\": prev_end,\n",
    "                \"text\": \" \".join(current_text).strip()\n",
    "            })\n",
    "            # abrir nuevo tramo\n",
    "            current_spk = w[\"speaker\"]\n",
    "            current_start = w[\"start\"]\n",
    "            current_text = [w.get(\"word\",\"\")]\n",
    "        else:\n",
    "            current_text.append(w.get(\"word\",\"\"))\n",
    "        prev_end = w[\"end\"]\n",
    "    # cerrar último\n",
    "    segs.append({\n",
    "        \"speaker\": current_spk,\n",
    "        \"start\": current_start,\n",
    "        \"end\": prev_end,\n",
    "        \"text\": \" \".join(current_text).strip()\n",
    "    })\n",
    "\n",
    "# Unir tramos contiguos del mismo speaker\n",
    "turns = merge_contiguous_turns(sorted(segs, key=lambda x: (x[\"start\"], x[\"end\"])))\n",
    "\n",
    "# Estadísticas por speaker\n",
    "speaker_stats = {}\n",
    "for t in turns:\n",
    "    d = t[\"end\"] - t[\"start\"]\n",
    "    spk = t[\"speaker\"]\n",
    "    if spk not in speaker_stats:\n",
    "        speaker_stats[spk] = {\"total_sec\":0.0, \"num_utts\":0}\n",
    "    speaker_stats[spk][\"total_sec\"] += max(0.0, d)\n",
    "    speaker_stats[spk][\"num_utts\"] += 1\n",
    "\n",
    "# Detectar entrevistador\n",
    "interviewer = guess_interviewer(speaker_stats, turns)\n",
    "\n",
    "# Marcar entrevistador en stats\n",
    "speakers_list = []\n",
    "for spk, st in sorted(speaker_stats.items()):\n",
    "    speakers_list.append({\n",
    "        \"id\": spk,\n",
    "        \"total_sec\": round(st[\"total_sec\"], 3),\n",
    "        \"num_utts\": st[\"num_utts\"],\n",
    "        \"is_interviewer\": (spk == interviewer)\n",
    "    })\n",
    "\n",
    "# Construir pares Q→A\n",
    "qa = build_qa(turns, interviewer)\n",
    "\n",
    "# Metadata\n",
    "duration = get_duration_sec(audio_path) or 0\n",
    "result_json = {\n",
    "    \"meta\": {\n",
    "        \"source_audio\": os.path.abspath(audio_path),\n",
    "        \"language\": asr_result.get(\"language\", args.lang),\n",
    "        \"duration_sec\": round(duration, 3),\n",
    "        \"created_utc\": datetime.datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\",\n",
    "        \"toolchain\": {\n",
    "            \"asr\": f\"whisperx-{args.model}\",\n",
    "            \"diarization\": \"pyannote\",\n",
    "            \"alignment\": \"mfa\"\n",
    "        }\n",
    "    },\n",
    "    \"speakers\": speakers_list,\n",
    "    \"turns\": [\n",
    "        {\n",
    "            \"speaker\": t[\"speaker\"],\n",
    "            \"start\": round(t[\"start\"], 3),\n",
    "            \"end\": round(t[\"end\"], 3),\n",
    "            \"text\": t[\"text\"]\n",
    "        } for t in turns if t[\"text\"]\n",
    "    ],\n",
    "    \"qa\": qa\n",
    "}\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Listo: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
